#!/usr/bin/env python3
"""
ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯• - å­¦ä¹ ï¼šæ€§èƒ½ç›‘æ§ã€ç“¶é¢ˆåˆ†æ
"""

import requests
import time
import threading
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed

class PerformanceBenchmark:
    def __init__(self, base_url="http://localhost:5000"):
        self.base_url = base_url
        self.results = {}
    
    def test_single_request(self, endpoint="/"):
        """æµ‹è¯•å•ä¸ªè¯·æ±‚æ€§èƒ½"""
        start_time = time.time()
        try:
            response = requests.get(f"{self.base_url}{endpoint}", timeout=10)
            response_time = (time.time() - start_time) * 1000  # è½¬æ¯«ç§’
            return {
                "status": "success",
                "status_code": response.status_code,
                "response_time": response_time,
                "content_length": len(response.content)
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "response_time": (time.time() - start_time) * 1000
            }
    
    def test_concurrent_requests(self, endpoint="/", num_requests=50, concurrency=10):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print(f"ğŸš€ æµ‹è¯•å¹¶å‘æ€§èƒ½: {num_requests}è¯·æ±‚, {concurrency}å¹¶å‘")
        
        times = []
        successes = 0
        errors = 0
        
        def worker():
            result = self.test_single_request(endpoint)
            return result
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = [executor.submit(worker) for _ in range(num_requests)]
            
            for future in as_completed(futures):
                result = future.result()
                times.append(result["response_time"])
                
                if result["status"] == "success":
                    successes += 1
                else:
                    errors += 1
        
        total_time = time.time() - start_time
        
        stats = {
            "total_requests": num_requests,
            "concurrency": concurrency,
            "successes": successes,
            "errors": errors,
            "success_rate": (successes / num_requests) * 100,
            "total_time": total_time,
            "requests_per_second": num_requests / total_time,
            "response_times": {
                "min": min(times) if times else 0,
                "max": max(times) if times else 0,
                "avg": statistics.mean(times) if times else 0,
                "p95": statistics.quantiles(times, n=20)[18] if len(times) >= 20 else 0
            }
        }
        
        return stats
    
    def test_different_endpoints(self):
        """æµ‹è¯•ä¸åŒç«¯ç‚¹çš„æ€§èƒ½"""
        endpoints = [
            ("é¦–é¡µ", "/"),
            ("æ–‡ç« é¡µ", "/post/1"),
            ("åˆ†ç±»ç®¡ç†", "/categories"),
            ("ç”¨æˆ·æ³¨å†Œ", "/register"),
        ]
        
        print("ğŸ“Š æµ‹è¯•ä¸åŒç«¯ç‚¹æ€§èƒ½...")
        print("=" * 60)
        
        for name, endpoint in endpoints:
            print(f"\næµ‹è¯•: {name} ({endpoint})")
            stats = self.test_concurrent_requests(endpoint, num_requests=20, concurrency=5)
            
            print(f"  æˆåŠŸç‡: {stats['success_rate']:.1f}%")
            print(f"  QPS: {stats['requests_per_second']:.1f} è¯·æ±‚/ç§’")
            print(f"  å¹³å‡å“åº”: {stats['response_times']['avg']:.1f}ms")
            print(f"  P95å“åº”: {stats['response_times']['p95']:.1f}ms")
            
            self.results[name] = stats
        
        return self.results
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ¯ æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        if not self.results:
            print("æ²¡æœ‰æµ‹è¯•æ•°æ®")
            return
        
        # æ€»ä½“ç»Ÿè®¡
        total_requests = sum(stats["total_requests"] for stats in self.results.values())
        total_successes = sum(stats["successes"] for stats in self.results.values())
        avg_qps = statistics.mean(stats["requests_per_second"] for stats in self.results.values())
        avg_response = statistics.mean(stats["response_times"]["avg"] for stats in self.results.values())
        
        print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"æ€»æˆåŠŸç‡: {(total_successes/total_requests)*100:.1f}%")
        print(f"å¹³å‡QPS: {avg_qps:.1f}")
        print(f"å¹³å‡å“åº”: {avg_response:.1f}ms")
        
        # æ€§èƒ½è¯„çº§
        if avg_response < 50:
            rating = "ğŸ‰ ä¼˜ç§€"
        elif avg_response < 100:
            rating = "âœ… è‰¯å¥½" 
        elif avg_response < 200:
            rating = "âš ï¸  ä¸€èˆ¬"
        else:
            rating = "âŒ éœ€è¦ä¼˜åŒ–"
        
        print(f"æ€§èƒ½è¯„çº§: {rating}")
        
        return self.results

if __name__ == '__main__':
    benchmark = PerformanceBenchmark()
    
    # æµ‹è¯•ä¸åŒç«¯ç‚¹
    benchmark.test_different_endpoints()
    
    # ç”ŸæˆæŠ¥å‘Š
    results = benchmark.generate_report()
    
    # ä¿å­˜ç»“æœï¼ˆç”¨äºåç»­å¯¹æ¯”ï¼‰
    with open("performance_results.json", "w") as f:
        import json
        json.dump(results, f, indent=2)
    
    print("\nğŸ’¡ ç»“æœå·²ä¿å­˜åˆ° performance_results.json")
